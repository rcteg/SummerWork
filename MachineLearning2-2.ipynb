{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Part 2-2:\n",
    "### Categorical Data Startup:\n",
    "#### 1: Imports pandas and the datasets, define your y or target as the SalePrice\n",
    "\n",
    "#### 2: Drop any x values that don't have SalePrice data (rows)\n",
    "\n",
    "#### 3: Create a list of missing columns\n",
    "\n",
    "#### 4: Creates a candidate predictor that drops the ID and SalePrice column from the training predictor set, and drops ID from the test predictor set(why do this?)\n",
    "\n",
    "#### 5: Define the low_cardinality_cols list as columns that have less than 10 potential values for the strings and ARE strings (dtype = object)\n",
    "\n",
    "#### 6: Define the numeric_cols list as columns that are type int64 or float64\n",
    "\n",
    "#### 7: Add the two lists together to create the train and test predictors that will be used\n",
    "\n",
    "#### 8: Print a sample of the dtypes of different columns just to show which are objects (strings) and which are ints or floats. This shows us how to identify types, split data, and adjust for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "BsmtHalfBath     int64\n",
      "OverallQual      int64\n",
      "KitchenAbvGr     int64\n",
      "MSSubClass       int64\n",
      "PavedDrive      object\n",
      "YearRemodAdd     int64\n",
      "BsmtFinSF2       int64\n",
      "RoofMatl        object\n",
      "BsmtFullBath     int64\n",
      "YearBuilt        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "traindata = pd.read_csv('train.csv')\n",
    "testdata = pd.read_csv('test.csv') #imports and declaring datasets\n",
    "\n",
    "traindata.dropna(axis='rows', subset=['SalePrice'], inplace=True)\n",
    "target = traindata.SalePrice #drops any rows that dont have SalePrice and declare the target (y value) as the SalePrice column\n",
    "\n",
    "cols_with_missing = []\n",
    "for i in traindata.columns:\n",
    "    if traindata[i].isnull().any():\n",
    "        cols_with_missing.append(i)\n",
    "print(cols_with_missing) #prints the list of columns that had missing values\n",
    "\n",
    "candidate_train_predictors = traindata.drop(['Id', 'SalePrice'] + cols_with_missing, axis= 'columns')\n",
    "candidate_test_predictors = testdata.drop(['Id'] + cols_with_missing, axis= 'columns') #Drop the SalePrice and Id columns from the data since it wouldn't work, as well as the columns with missing values\n",
    "\n",
    "low_cardinality_cols = []\n",
    "numeric_cols = []\n",
    "for i in candidate_train_predictors.columns:\n",
    "    if candidate_train_predictors[i].nunique() <10 and candidate_train_predictors[i].dtype == 'object':\n",
    "        low_cardinality_cols.append(i)\n",
    "    if candidate_train_predictors[i].dtype in ['int64', 'float64']:\n",
    "        numeric_cols.append(i)\n",
    "my_cols = low_cardinality_cols + numeric_cols #determine every low cardinality string (object) and numeric column, and add it to its respective list. Then add them together to determine every column that will be used\n",
    "\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols] #divide both the training and testing sets into the my_cols that will be used\n",
    "\n",
    "print(train_predictors.dtypes.sample(10)) #print a sample of the types of 10 random columns that will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the effectiveness of hot encoding predictors as opposed to dropping columns\n",
    "#### 9: Import cross_val_score for later calculating the MAE, as well as the model\n",
    "\n",
    "#### 10: Use pd.get_dummies on train_predictors to hot encode columns (separate each column into every possible value as a 1 or 0, which avoids having to deal with any strings and instead makes it categorical number data, which the imputer can handle.\n",
    "\n",
    "#### 11: Define a get_mae method that uses cross_val_score (I'm going to be honest, they just threw this at us without explaining anything) to cross validate the model multiple times, then we take the mean of these to find the MAE. Don't know why we didn't use the regular method.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "#### 12: Creates a list of predictors that removes any objects (strings)\n",
    "\n",
    "#### 13: Tests the mae of the models with and without the string predictors, with the hot encoded values performing better. This is because it accounts for whether or not the values were imputed (and therefore not actual data, subject to biases or inaccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
      "0             60     8450            7            5       2003          2003   \n",
      "1             20     9600            6            8       1976          1976   \n",
      "2             60    11250            7            5       2001          2002   \n",
      "3             70     9550            7            5       1915          1970   \n",
      "4             60    14260            8            5       2000          2000   \n",
      "5             50    14115            5            5       1993          1995   \n",
      "6             20    10084            8            5       2004          2005   \n",
      "7             60    10382            7            6       1973          1973   \n",
      "8             50     6120            7            5       1931          1950   \n",
      "9            190     7420            5            6       1939          1950   \n",
      "10            20    11200            5            5       1965          1965   \n",
      "11            60    11924            9            5       2005          2006   \n",
      "12            20    12968            5            6       1962          1962   \n",
      "13            20    10652            7            5       2006          2007   \n",
      "14            20    10920            6            5       1960          1960   \n",
      "15            45     6120            7            8       1929          2001   \n",
      "16            20    11241            6            7       1970          1970   \n",
      "17            90    10791            4            5       1967          1967   \n",
      "18            20    13695            5            5       2004          2004   \n",
      "19            20     7560            5            6       1958          1965   \n",
      "20            60    14215            8            5       2005          2006   \n",
      "21            45     7449            7            7       1930          1950   \n",
      "22            20     9742            8            5       2002          2002   \n",
      "23           120     4224            5            7       1976          1976   \n",
      "24            20     8246            5            8       1968          2001   \n",
      "25            20    14230            8            5       2007          2007   \n",
      "26            20     7200            5            7       1951          2000   \n",
      "27            20    11478            8            5       2007          2008   \n",
      "28            20    16321            5            6       1957          1997   \n",
      "29            30     6324            4            6       1927          1950   \n",
      "...          ...      ...          ...          ...        ...           ...   \n",
      "1430          60    21930            5            5       2005          2005   \n",
      "1431         120     4928            6            6       1976          1976   \n",
      "1432          30    10800            4            6       1927          2007   \n",
      "1433          60    10261            6            5       2000          2000   \n",
      "1434          20    17400            5            5       1977          1977   \n",
      "1435          20     8400            6            9       1962          2005   \n",
      "1436          20     9000            4            6       1971          1971   \n",
      "1437          20    12444            8            5       2008          2008   \n",
      "1438          20     7407            6            7       1957          1996   \n",
      "1439          60    11584            7            6       1979          1979   \n",
      "1440          70    11526            6            7       1922          1994   \n",
      "1441         120     4426            6            5       2004          2004   \n",
      "1442          60    11003           10            5       2008          2008   \n",
      "1443          30     8854            6            6       1916          1950   \n",
      "1444          20     8500            7            5       2004          2004   \n",
      "1445          85     8400            6            5       1966          1966   \n",
      "1446          20    26142            5            7       1962          1962   \n",
      "1447          60    10000            8            5       1995          1996   \n",
      "1448          50    11767            4            7       1910          2000   \n",
      "1449         180     1533            5            7       1970          1970   \n",
      "1450          90     9000            5            5       1974          1974   \n",
      "1451          20     9262            8            5       2008          2009   \n",
      "1452         180     3675            5            5       2005          2005   \n",
      "1453          20    17217            5            5       2006          2006   \n",
      "1454          20     7500            7            5       2004          2005   \n",
      "1455          60     7917            6            5       1999          2000   \n",
      "1456          20    13175            6            6       1978          1988   \n",
      "1457          70     9042            7            9       1941          2006   \n",
      "1458          20     9717            5            6       1950          1996   \n",
      "1459          20     9937            5            6       1965          1965   \n",
      "\n",
      "      BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
      "0            706           0        150          856          ...             \n",
      "1            978           0        284         1262          ...             \n",
      "2            486           0        434          920          ...             \n",
      "3            216           0        540          756          ...             \n",
      "4            655           0        490         1145          ...             \n",
      "5            732           0         64          796          ...             \n",
      "6           1369           0        317         1686          ...             \n",
      "7            859          32        216         1107          ...             \n",
      "8              0           0        952          952          ...             \n",
      "9            851           0        140          991          ...             \n",
      "10           906           0        134         1040          ...             \n",
      "11           998           0        177         1175          ...             \n",
      "12           737           0        175          912          ...             \n",
      "13             0           0       1494         1494          ...             \n",
      "14           733           0        520         1253          ...             \n",
      "15             0           0        832          832          ...             \n",
      "16           578           0        426         1004          ...             \n",
      "17             0           0          0            0          ...             \n",
      "18           646           0        468         1114          ...             \n",
      "19           504           0        525         1029          ...             \n",
      "20             0           0       1158         1158          ...             \n",
      "21             0           0        637          637          ...             \n",
      "22             0           0       1777         1777          ...             \n",
      "23           840           0        200         1040          ...             \n",
      "24           188         668        204         1060          ...             \n",
      "25             0           0       1566         1566          ...             \n",
      "26           234         486        180          900          ...             \n",
      "27          1218           0        486         1704          ...             \n",
      "28          1277           0        207         1484          ...             \n",
      "29             0           0        520          520          ...             \n",
      "...          ...         ...        ...          ...          ...             \n",
      "1430           0           0        732          732          ...             \n",
      "1431         958           0          0          958          ...             \n",
      "1432           0           0        656          656          ...             \n",
      "1433           0           0        936          936          ...             \n",
      "1434         936           0        190         1126          ...             \n",
      "1435           0           0       1319         1319          ...             \n",
      "1436         616           0        248          864          ...             \n",
      "1437        1336           0        596         1932          ...             \n",
      "1438         600           0        312          912          ...             \n",
      "1439         315         110        114          539          ...             \n",
      "1440           0           0        588          588          ...             \n",
      "1441         697           0        151          848          ...             \n",
      "1442         765           0        252         1017          ...             \n",
      "1443           0           0        952          952          ...             \n",
      "1444           0           0       1422         1422          ...             \n",
      "1445         187         627          0          814          ...             \n",
      "1446         593           0        595         1188          ...             \n",
      "1447        1079           0        141         1220          ...             \n",
      "1448           0           0        560          560          ...             \n",
      "1449         553           0         77          630          ...             \n",
      "1450           0           0        896          896          ...             \n",
      "1451           0           0       1573         1573          ...             \n",
      "1452         547           0          0          547          ...             \n",
      "1453           0           0       1140         1140          ...             \n",
      "1454         410           0        811         1221          ...             \n",
      "1455           0           0        953          953          ...             \n",
      "1456         790         163        589         1542          ...             \n",
      "1457         275           0        877         1152          ...             \n",
      "1458          49        1029          0         1078          ...             \n",
      "1459         830         290        136         1256          ...             \n",
      "\n",
      "      SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
      "0                  0             0             0            1   \n",
      "1                  0             0             0            1   \n",
      "2                  0             0             0            1   \n",
      "3                  0             0             0            1   \n",
      "4                  0             0             0            1   \n",
      "5                  0             0             0            1   \n",
      "6                  0             0             0            1   \n",
      "7                  0             0             0            1   \n",
      "8                  0             0             0            1   \n",
      "9                  0             0             0            1   \n",
      "10                 0             0             0            1   \n",
      "11                 0             1             0            0   \n",
      "12                 0             0             0            1   \n",
      "13                 0             1             0            0   \n",
      "14                 0             0             0            1   \n",
      "15                 0             0             0            1   \n",
      "16                 0             0             0            1   \n",
      "17                 0             0             0            1   \n",
      "18                 0             0             0            1   \n",
      "19                 0             0             0            0   \n",
      "20                 0             1             0            0   \n",
      "21                 0             0             0            1   \n",
      "22                 0             0             0            1   \n",
      "23                 0             0             0            1   \n",
      "24                 0             0             0            1   \n",
      "25                 0             0             0            1   \n",
      "26                 0             0             0            1   \n",
      "27                 0             0             0            1   \n",
      "28                 0             0             0            1   \n",
      "29                 0             0             0            1   \n",
      "...              ...           ...           ...          ...   \n",
      "1430               0             0             0            1   \n",
      "1431               0             0             0            1   \n",
      "1432               0             0             0            1   \n",
      "1433               0             0             0            1   \n",
      "1434               0             0             0            1   \n",
      "1435               0             0             0            0   \n",
      "1436               0             0             0            1   \n",
      "1437               0             1             0            0   \n",
      "1438               0             0             0            1   \n",
      "1439               0             0             0            1   \n",
      "1440               0             0             0            1   \n",
      "1441               0             0             0            1   \n",
      "1442               0             0             0            1   \n",
      "1443               0             0             0            1   \n",
      "1444               0             0             0            1   \n",
      "1445               0             0             0            1   \n",
      "1446               0             0             0            1   \n",
      "1447               0             0             0            1   \n",
      "1448               0             0             0            1   \n",
      "1449               0             0             0            1   \n",
      "1450               0             0             0            1   \n",
      "1451               0             1             0            0   \n",
      "1452               0             0             0            1   \n",
      "1453               0             0             0            1   \n",
      "1454               0             0             0            1   \n",
      "1455               0             0             0            1   \n",
      "1456               0             0             0            1   \n",
      "1457               0             0             0            1   \n",
      "1458               0             0             0            1   \n",
      "1459               0             0             0            1   \n",
      "\n",
      "      SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
      "0                         0                      0                     0   \n",
      "1                         0                      0                     0   \n",
      "2                         0                      0                     0   \n",
      "3                         1                      0                     0   \n",
      "4                         0                      0                     0   \n",
      "5                         0                      0                     0   \n",
      "6                         0                      0                     0   \n",
      "7                         0                      0                     0   \n",
      "8                         1                      0                     0   \n",
      "9                         0                      0                     0   \n",
      "10                        0                      0                     0   \n",
      "11                        0                      0                     0   \n",
      "12                        0                      0                     0   \n",
      "13                        0                      0                     0   \n",
      "14                        0                      0                     0   \n",
      "15                        0                      0                     0   \n",
      "16                        0                      0                     0   \n",
      "17                        0                      0                     0   \n",
      "18                        0                      0                     0   \n",
      "19                        1                      0                     0   \n",
      "20                        0                      0                     0   \n",
      "21                        0                      0                     0   \n",
      "22                        0                      0                     0   \n",
      "23                        0                      0                     0   \n",
      "24                        0                      0                     0   \n",
      "25                        0                      0                     0   \n",
      "26                        0                      0                     0   \n",
      "27                        0                      0                     0   \n",
      "28                        0                      0                     0   \n",
      "29                        0                      0                     0   \n",
      "...                     ...                    ...                   ...   \n",
      "1430                      0                      0                     0   \n",
      "1431                      0                      0                     0   \n",
      "1432                      0                      0                     0   \n",
      "1433                      0                      0                     0   \n",
      "1434                      0                      0                     0   \n",
      "1435                      1                      0                     0   \n",
      "1436                      0                      0                     0   \n",
      "1437                      0                      0                     0   \n",
      "1438                      0                      0                     0   \n",
      "1439                      0                      0                     0   \n",
      "1440                      0                      0                     0   \n",
      "1441                      0                      0                     0   \n",
      "1442                      0                      0                     0   \n",
      "1443                      0                      0                     0   \n",
      "1444                      0                      0                     0   \n",
      "1445                      0                      0                     0   \n",
      "1446                      0                      0                     0   \n",
      "1447                      0                      0                     0   \n",
      "1448                      0                      0                     0   \n",
      "1449                      1                      0                     0   \n",
      "1450                      0                      0                     0   \n",
      "1451                      0                      0                     0   \n",
      "1452                      0                      0                     0   \n",
      "1453                      1                      0                     0   \n",
      "1454                      0                      0                     0   \n",
      "1455                      0                      0                     0   \n",
      "1456                      0                      0                     0   \n",
      "1457                      0                      0                     0   \n",
      "1458                      0                      0                     0   \n",
      "1459                      0                      0                     0   \n",
      "\n",
      "      SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
      "0                        0                     1                      0  \n",
      "1                        0                     1                      0  \n",
      "2                        0                     1                      0  \n",
      "3                        0                     0                      0  \n",
      "4                        0                     1                      0  \n",
      "5                        0                     1                      0  \n",
      "6                        0                     1                      0  \n",
      "7                        0                     1                      0  \n",
      "8                        0                     0                      0  \n",
      "9                        0                     1                      0  \n",
      "10                       0                     1                      0  \n",
      "11                       0                     0                      1  \n",
      "12                       0                     1                      0  \n",
      "13                       0                     0                      1  \n",
      "14                       0                     1                      0  \n",
      "15                       0                     1                      0  \n",
      "16                       0                     1                      0  \n",
      "17                       0                     1                      0  \n",
      "18                       0                     1                      0  \n",
      "19                       0                     0                      0  \n",
      "20                       0                     0                      1  \n",
      "21                       0                     1                      0  \n",
      "22                       0                     1                      0  \n",
      "23                       0                     1                      0  \n",
      "24                       0                     1                      0  \n",
      "25                       0                     1                      0  \n",
      "26                       0                     1                      0  \n",
      "27                       0                     1                      0  \n",
      "28                       0                     1                      0  \n",
      "29                       0                     1                      0  \n",
      "...                    ...                   ...                    ...  \n",
      "1430                     0                     1                      0  \n",
      "1431                     0                     1                      0  \n",
      "1432                     0                     1                      0  \n",
      "1433                     0                     1                      0  \n",
      "1434                     0                     1                      0  \n",
      "1435                     0                     0                      0  \n",
      "1436                     0                     1                      0  \n",
      "1437                     0                     0                      1  \n",
      "1438                     0                     1                      0  \n",
      "1439                     0                     1                      0  \n",
      "1440                     0                     1                      0  \n",
      "1441                     0                     1                      0  \n",
      "1442                     0                     1                      0  \n",
      "1443                     0                     1                      0  \n",
      "1444                     0                     1                      0  \n",
      "1445                     0                     1                      0  \n",
      "1446                     0                     1                      0  \n",
      "1447                     0                     1                      0  \n",
      "1448                     0                     1                      0  \n",
      "1449                     0                     0                      0  \n",
      "1450                     0                     1                      0  \n",
      "1451                     0                     0                      1  \n",
      "1452                     0                     1                      0  \n",
      "1453                     0                     0                      0  \n",
      "1454                     0                     1                      0  \n",
      "1455                     0                     1                      0  \n",
      "1456                     0                     1                      0  \n",
      "1457                     0                     1                      0  \n",
      "1458                     0                     1                      0  \n",
      "1459                     0                     1                      0  \n",
      "\n",
      "[1460 rows x 159 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18549.00005\n",
      "Mean Abslute Error with One-Hot Encoding: 17900.4435013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors) #pd.get_dummies method to hot encode the train columns\n",
    "\n",
    "print(one_hot_encoded_training_predictors)\n",
    "\n",
    "def get_mae(x, y):\n",
    "    return -1* cross_val_score(RandomForestRegressor(50), x, y, scoring = 'neg_mean_absolute_error').mean() #THEY DIDN'T EXPLAIN THIS AT ALL, but it essentially calculates the MAE\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude = ['object']) #exclude string columns as a whole\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target) #calculate the mae without the string columns\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target) #calculate the mae with string columns\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(mae_without_categoricals))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(mae_one_hot_encoded)) #print the MAE comparison\n",
    "\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors) # hot encode the test columns\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, join='left', axis='columns')\n",
    "#aligns the train and test predictors. left means it keeps columns found in training, join = 'inner' can also work (only includes columns found in both), specifying the axis as the columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model: A useful model that requires much tuning, but is more accurate. It is an 'ensemble' model, meaning that it uses predictions and its error to successively build more and more accurate models.\n",
    "#### 14: Import the necessary nethods and packages\n",
    "\n",
    "#### 15: Drop the columns with NaN values, declare the y value as SalePrice, and drop SalePrice from the x values\n",
    "\n",
    "#### 16: Split the data, impute it, and train the model with it. The model performs very well,  with a relatively low error compared to random forest and especially decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 18555.9380137\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer #imports necessary methods and packages\n",
    "\n",
    "data = pd.read_csv('train.csv') #create dataframe\n",
    "data.dropna(axis = 'rows', subset = ['SalePrice'], inplace = True) #drops any columns with NaN values\n",
    "y = data.SalePrice #define the y value as the sale price\n",
    "x = data.drop(['SalePrice'], axis = 'columns').select_dtypes(exclude = ['object']) #drop SalePrice from the x values\n",
    "\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size = 0.25) #split into 25% test data and 75% train data\n",
    "myimputer = Imputer() \n",
    "train_x = myimputer.fit_transform(train_x)\n",
    "val_x = myimputer.fit_transform(val_x)#impute the missing values in both the training set and the actual values\n",
    "\n",
    "from xgboost import XGBRegressor #import a gradient booster\n",
    "\n",
    "mymodel = XGBRegressor()\n",
    "mymodel.fit(train_x, train_y, verbose = False) #define a model, train it, and make verbose false (will not print stuff)\n",
    "\n",
    "predictions = mymodel.predict(val_x)# create predictions from the model\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error: %s\"%(mean_absolute_error(val_y, predictions)) ) #calculate the MAE from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together to create a better prediction:\n",
    "#### 17: import the relevant method and packages\n",
    "\n",
    "#### 18: Create the data and testdata DataFrames\n",
    "\n",
    "#### 19: Drop any rows that don't have a SalePrice value for data, then declare the y.\n",
    "\n",
    "#### 20: Identify columns that have empty values, add it to the list columns_with_empty, and print the sum of the NaN values for each column.\n",
    "\n",
    "#### 21: I DID drop columns with an overwhelming amount of NaN values (over half of the column) from both the data and testdata, as I thought that they will be largely useless and imputed values will not have enough data points to make an accurate value. However, something like the quality of a pool can imply that there *is* a pool (imputation with extension), and could reasonably have an impact on the value. The submission confirmed this, so these lines were commented out.\n",
    "\n",
    "#### 22: Drop SalePrice from the 'data' Dataframe, the 'x' values.\n",
    "\n",
    "#### 23: Determine the columns that have a String and low cardinality or are floats or ints, adding them to the list total_cols\n",
    "\n",
    "#### 24: Take only the total_cols from the dataset and make it the new data AND testdata.\n",
    "\n",
    "#### 25: Adds a column_was_missing for any column in the columns_with_empty list.\n",
    "\n",
    "#### 26: Creates 'dummy' columns for every categorical data value. \n",
    "\n",
    "#### 27: 'Aligns' the values for the training and test data. This uses 'inner', meaning that it will only keep columns that show up in both sets.  This avoids errors building the model.\n",
    "\n",
    "#### 28 Splits the data into a testing and training set in order to optimize the model and prevent bias/interpolation. The split is 80% train 20% test.\n",
    "\n",
    "#### 29: Imputes the values of the entire dataset, which returns a NumPy array. Because the array is not a DataFrame, and we need a DataFrame for other operations, we have to convert both the data and testdata sets to DataFrames by using the indices and columns that were in the original DataFrame before it was imputed.\n",
    "\n",
    "#### 29: Goes through a large amount of n_estimator values, and the model already has other important values set such as early_stopping_rounds (how many decreases in MAE need to occur for it to automatically stop), the amount of cores that will be used to parallel process, and the learning rate (which is 1/10 of what it usually is, making a more accurate but much less efficient to process model). The for loop will determine the minimum MAE and use that in the model that will predict the actual results that will be submitted.\n",
    "\n",
    "#### 30: Creates a new model using the results of the previous for loop, fits it to the train data(should I use the entire training set or what I used for the optimization, 80%? Test later) and predicts values based upon the testdata.\n",
    "\n",
    "#### 31: Creates a csv submission file and prints it to the console. However, the 'Id' column was not in the form it needed to be in, so we have to use the astype() method to convert it into an int32, what it was expecting. This csv can be uploaded to Kaggle for scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GarageYrBlt       81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GarageYrBlt       81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "At 5050 n_estimators, the MAE was 16769.475746\n",
      "The best MAE was 16769.475746 at 5050 n_estimators\n",
      "        Id      SalePrice\n",
      "0     1461  120632.703125\n",
      "1     1462  167893.500000\n",
      "2     1463  179617.500000\n",
      "3     1464  190530.546875\n",
      "4     1465  183308.937500\n",
      "5     1466  174320.265625\n",
      "6     1467  166762.843750\n",
      "7     1468  163340.921875\n",
      "8     1469  186366.125000\n",
      "9     1470  126764.281250\n",
      "10    1471  196604.406250\n",
      "11    1472   90407.515625\n",
      "12    1473   91495.296875\n",
      "13    1474  154941.328125\n",
      "14    1475  134715.171875\n",
      "15    1476  395660.312500\n",
      "16    1477  252738.656250\n",
      "17    1478  303216.000000\n",
      "18    1479  215076.687500\n",
      "19    1480  529295.312500\n",
      "20    1481  309306.656250\n",
      "21    1482  213535.968750\n",
      "22    1483  165042.500000\n",
      "23    1484  162428.546875\n",
      "24    1485  174679.343750\n",
      "25    1486  189410.218750\n",
      "26    1487  369438.843750\n",
      "27    1488  222105.265625\n",
      "28    1489  197827.750000\n",
      "29    1490  221780.109375\n",
      "...    ...            ...\n",
      "1429  2890   84550.210938\n",
      "1430  2891  138660.734375\n",
      "1431  2892   40711.746094\n",
      "1432  2893   86975.656250\n",
      "1433  2894   50204.277344\n",
      "1434  2895  324236.937500\n",
      "1435  2896  283346.843750\n",
      "1436  2897  205595.093750\n",
      "1437  2898  148511.781250\n",
      "1438  2899  219942.406250\n",
      "1439  2900  162892.875000\n",
      "1440  2901  218061.546875\n",
      "1441  2902  193773.890625\n",
      "1442  2903  339452.687500\n",
      "1443  2904  362532.062500\n",
      "1444  2905   86330.382812\n",
      "1445  2906  210451.265625\n",
      "1446  2907  110414.046875\n",
      "1447  2908  132703.171875\n",
      "1448  2909  137055.984375\n",
      "1449  2910   78070.273438\n",
      "1450  2911   80613.468750\n",
      "1451  2912  142842.312500\n",
      "1452  2913   69328.250000\n",
      "1453  2914   67579.421875\n",
      "1454  2915   77029.382812\n",
      "1455  2916   70855.992188\n",
      "1456  2917  164125.593750\n",
      "1457  2918  116880.242188\n",
      "1458  2919  239823.968750\n",
      "\n",
      "[1459 rows x 2 columns]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "testdata = pd.read_csv('test.csv')\n",
    "\n",
    "data.dropna(axis = 'rows', subset = ['SalePrice'], inplace = True)\n",
    "y = data.SalePrice\n",
    "\n",
    "columns_with_empty = []\n",
    "for i in data.columns:\n",
    "    if data[i].isnull().any():\n",
    "        columns_with_empty.append(i)\n",
    "print(data[columns_with_empty].isnull().sum())\n",
    "\n",
    "#mostlyMissing = ['Alley', 'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu']\n",
    "\n",
    "#data = data.drop(mostlyMissing, axis = 'columns')\n",
    "#testdata = testdata.drop(mostlyMissing, axis ='columns')\n",
    "\n",
    "#columns_with_empty = list(set(columns_with_empty) - set(mostlyMissing))\n",
    "print(data[columns_with_empty].isnull().sum())\n",
    "\n",
    "data = data.drop(['SalePrice'], axis = 'columns')\n",
    "\n",
    "total_cols = []\n",
    "for i in data.columns:\n",
    "    if (data[i].nunique() <10 and data[i].dtype == 'object') or data[i].dtype in ['int64', 'float64']:\n",
    "        total_cols.append(i)\n",
    "data = data[total_cols]\n",
    "testdata = testdata[total_cols]\n",
    "\n",
    "for i in columns_with_empty:\n",
    "    data[i + '_was_empty'] = data[i].isnull()\n",
    "    testdata[i + 'was_empty'] = testdata[i].isnull()\n",
    "    \n",
    "ohetraining = pd.get_dummies(data)\n",
    "ohetest = pd.get_dummies(testdata)\n",
    "\n",
    "ohetraining, ohetest = ohetraining.align(ohetest, join = 'inner', axis = 'columns')\n",
    "\n",
    "my_imputer = Imputer()\n",
    "\n",
    "ohetraining2 = my_imputer.fit_transform(ohetraining)\n",
    "ohetest2 = my_imputer.fit_transform(ohetest)\n",
    "\n",
    "imputed_features_df_train = pd.DataFrame(ohetraining2, index = ohetraining.index, columns = ohetraining.columns)\n",
    "imputed_features_df_test = pd.DataFrame(ohetest2, index = ohetest.index, columns = ohetest.columns)\n",
    "\n",
    "imputed_features_df_train, imputed_features_df_test = imputed_features_df_train.align(imputed_features_df_test, join = 'inner', axis = 'columns')\n",
    "\n",
    "#print(imputed_features_df_train.columns)\n",
    "#print(imputed_features_df_test.columns)\n",
    "\n",
    "train_x, tval_x, train_y, tval_y = train_test_split(imputed_features_df_train, y, test_size = 0.2)\n",
    "\n",
    "#mymodel.fit(train_x, train_y, verbose = False)\n",
    "#predictions = mymodel.predict(tval_x)\n",
    "\n",
    "#print(mean_absolute_error(tval_y, predictions))\n",
    "\n",
    "n_estimatorsL = []\n",
    "for i in range(5050, 5100, 50):\n",
    "    n_estimatorsL.append(i)\n",
    "\n",
    "minNum = 9999999999999\n",
    "numVal = 0\n",
    "\n",
    "for i in n_estimatorsL:\n",
    "    mymodel = XGBRegressor(learning_rate = 0.01, n_estimators = i, early_stopping_rounds = 15, n_jobs =4)\n",
    "    mymodel.fit(train_x, train_y)\n",
    "    predictions = mymodel.predict(tval_x)\n",
    "    mae = mean_absolute_error(tval_y, predictions)\n",
    "    print('At %s n_estimators, the MAE was %2f'%(i, mae))\n",
    "    if mae < minNum:\n",
    "        minNum = mae\n",
    "        numVal = i\n",
    "print('The best MAE was %2f at %s n_estimators'%(minNum, numVal))\n",
    "\n",
    "mymodel2 = XGBRegressor(learning_rate = 0.01, n_estimators = numVal, early_stopping_rounds = 10, n_jobs =4)\n",
    "mymodel2.fit(imputed_features_df_train, y)\n",
    "\n",
    "predictions = mymodel2.predict(imputed_features_df_test)\n",
    "\n",
    "submission = pd.DataFrame({'Id':imputed_features_df_test.Id.astype('Int32'), 'SalePrice':predictions})\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "print(submission)\n",
    "print('done')\n",
    "    \n",
    "#one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "#final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, join='left', axis='columns')\n",
    "\n",
    "#Your Best Entry \n",
    "#You advanced 1,667 places on the leaderboard from 3599 to 1932!\n",
    "#Your submission scored 0.13723, which is an improvement of your previous score of 0.19069. Great job!\n",
    "#Your Best Entry \n",
    "#You advanced 12 places on the leaderboard 1932 to 1920!\n",
    "#Your submission scored 0.13687, which is an improvement of your previous score of 0.13723. Great job!\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plots: Examining the effect of a single variable on model predictions for insight\n",
    "#### 1: Import the necessary methods and libraries\n",
    "\n",
    "#### 2: Create the DataFrame, define the y, and print the columns of the DataFrame to see what features can be tested\n",
    "\n",
    "#### 3: The features that will be tested are the year the house was built and the area of the lot the house is on.  The data is imputed and turned back into a DataFrame.\n",
    "\n",
    "#### 4: The model is created, fit, and then splotted with the grid_resolution representing how many points are used to create the graph. As you can see, the price increases (in general) as the house becomes 'newer' or the area of the lot it is on expands. This makes sense, and implies that there is no leakage (for these variables)\n",
    "\n",
    "### Unsure how to connect this with XGB instead of sklearn (ValueError: gbrt has to be an instance of BaseGradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
      "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
      "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
      "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
      "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
      "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
      "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
      "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
      "       'SaleCondition', 'SalePrice'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "#import xgboost\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.preprocessing import Imputer\n",
    "data2 = pd.read_csv('train.csv')\n",
    "y = data2.SalePrice\n",
    "\n",
    "print(data2.columns)\n",
    "\n",
    "cols_to_use = ['YearBuilt', 'LotArea']\n",
    "\n",
    "my_imputer = Imputer()\n",
    "data2 = data2[cols_to_use]\n",
    "x = my_imputer.fit_transform(data2)\n",
    "x = pd.DataFrame(x, data2.index, data2.columns)\n",
    "\n",
    "mymodel = GradientBoostingRegressor()\n",
    "mymodel.fit(x,y)\n",
    "my_plots = plot_partial_dependence(mymodel, features = [0,1], X=x, feature_names = cols_to_use, grid_resolution = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines: A compact alternative to preprocessing and training\n",
    "#### 1: imports, etc\n",
    "\n",
    "#### 2: We do some basic preprocessing of dropping rows and columns we don't need/want, making x and y variables, and splitting them. \n",
    "\n",
    "#### 3: We then create the pipeline, adding an imputer and defining parameters of the XGBRegressor inside the creation of the pipeline by referring to the variable the pipeline creates called xgbregressor. \n",
    "\n",
    "#### 4: Fit the pipeline and print out the MAE!\n",
    "\n",
    "### Creating a pipeline like this reduces data leakage from imputing values before a split. Later, we can also include other transformers that will do things like OneHotEncode for us. Also, it may be that we can't actually edit the values of the XGBRegreessor like that, so I will have to figure that out in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15342.0729987\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "data = pd.read_csv('train.csv')\n",
    "data.dropna(axis = 'rows', subset = ['SalePrice'], inplace = True)\n",
    "y = data.SalePrice\n",
    "x = data.drop(['SalePrice'], axis = 'columns').select_dtypes(exclude = ['object'])\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "my_pipeline = make_pipeline(Imputer(), XGBRegressor(xgbregressor__learning_rate = 0.01, xgbregressor_n_estimators = 5000, xgbregressor__verbose= False))\n",
    "my_pipeline.fit(train_x, train_y)\n",
    "\n",
    "predictions = my_pipeline.predict(val_x)\n",
    "\n",
    "print(mean_absolute_error(val_y, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: Using multiple models across a dataset to provide more accurate predictions\n",
    "#### 1: Import packages and drop any rows from train data that don't have SalePrice data\n",
    "\n",
    "#### 2: Declare the y value for training and drop SalePrice\n",
    "\n",
    "#### 3: Go through every column and keep ones that are numbers or low cardinality strings (we don't want a million different options)\n",
    "\n",
    "#### 4: Add extension columns to show which values will be imputed\n",
    "\n",
    "#### 5: Hot encode the string columns to remove string data and make it categorical\n",
    "\n",
    "#### 6: Align the data and testdata to ensure that there are no extraneous columns\n",
    "\n",
    "#### 7: Create the pipeline and use cross validation (with 5 models running at once here, cv=5) to determine the MAE of this model.\n",
    "\n",
    "#### 8: Train the model and submit the predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
      "-16493.8101884\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "testdata = pd.read_csv('test.csv')\n",
    "data.dropna(axis = 'rows', subset = ['SalePrice'], inplace = True)\n",
    "y = data.SalePrice\n",
    "data = data.drop(['SalePrice'], axis = 'columns')\n",
    "\n",
    "total_cols = []\n",
    "for i in data.columns:\n",
    "    if (data[i].nunique() <10 and data[i].dtype == 'object') or data[i].dtype in ['int64', 'float64']:\n",
    "        total_cols.append(i)\n",
    "print(total_cols)\n",
    "data = data[total_cols]\n",
    "testdata = testdata[total_cols]\n",
    "\n",
    "for i in columns_with_empty:\n",
    "    data[i + '_was_empty'] = data[i].isnull()\n",
    "    testdata[i + 'was_empty'] = testdata[i].isnull()\n",
    "\n",
    "\n",
    "\n",
    "data = pd.get_dummies(data)\n",
    "testdata = pd.get_dummies(testdata)\n",
    "\n",
    "data, testdata = data.align(testdata, join = 'inner', axis = 'columns')\n",
    "\n",
    "x = data\n",
    "\n",
    "my_pipeline = make_pipeline(Imputer(), XGBRegressor(xgbregressor_learning_rate = 0.01, xgbregressor_n_estimators = 5050, xbgregressor_n_jobs = -1, xgbregressor_verbose = False))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(my_pipeline, x, y, scoring='neg_mean_absolute_error', n_jobs = -1, cv = 5)\n",
    "print(sum(scores)/5)\n",
    "\n",
    "my_pipeline.fit(data, y)\n",
    "\n",
    "predictions = my_pipeline.predict(testdata)\n",
    "submission = pd.DataFrame({'Id':testdata.Id.astype('Int32'), 'SalePrice':predictions})\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooray! We are now done with the Kaggle Machine Learning course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
