{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Regression from Kaggle\n",
    "#### 1: Importing the pandas library for use.\n",
    "\n",
    "#### 2: Create a DataFrame object from the main_file_path string using the pandas.read_csv() method.\n",
    "\n",
    "#### 3: Prints the columns of the dataset to show what variables are available for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "bd549f7ff2f25db046a954c8a97685ed533a50a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
      "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
      "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
      "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
      "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
      "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
      "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
      "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
      "       'SaleCondition', 'SalePrice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "main_file_path = 'train.csv' #set a string for the file path of your csv file\n",
    "data = pd.read_csv(main_file_path) #create a data object/variable from the file path string using the pandas read_csv method\n",
    "print(data.columns) #print the columns of the data using the column method in order to ascertain what variables there are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4:Uses dot notation to print out the head (first five rows) of the \"SalePrice\" column of our DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26198ed68c08447761f06891949e632142e94e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "saleprice_data = data.SalePrice #use dot notation (to get a field of the data object) that aligns with the colum you are looking for\n",
    "print(saleprice_data.head()) # print the top few entries of the data colum selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5: Creates a list of two \"features\" (x variables) that we will use for analysis from the list printed above in step 3.\n",
    "\n",
    "#### 6: Uses a list splicing method to select the values under the columns we will use for analysis.\n",
    "\n",
    "#### 7: Prints and gives a basic statistical analysis of the columns selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9099ed87b6fb0c82088419b75f8475829ca2eaec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Neighborhood  YearBuilt\n",
      "0         CollgCr       2003\n",
      "1         Veenker       1976\n",
      "2         CollgCr       2001\n",
      "3         Crawfor       1915\n",
      "4         NoRidge       2000\n",
      "5         Mitchel       1993\n",
      "6         Somerst       2004\n",
      "7          NWAmes       1973\n",
      "8         OldTown       1931\n",
      "9         BrkSide       1939\n",
      "10         Sawyer       1965\n",
      "11        NridgHt       2005\n",
      "12         Sawyer       1962\n",
      "13        CollgCr       2006\n",
      "14          NAmes       1960\n",
      "15        BrkSide       1929\n",
      "16          NAmes       1970\n",
      "17         Sawyer       1967\n",
      "18        SawyerW       2004\n",
      "19          NAmes       1958\n",
      "20        NridgHt       2005\n",
      "21         IDOTRR       1930\n",
      "22        CollgCr       2002\n",
      "23        MeadowV       1976\n",
      "24         Sawyer       1968\n",
      "25        NridgHt       2007\n",
      "26          NAmes       1951\n",
      "27        NridgHt       2007\n",
      "28          NAmes       1957\n",
      "29        BrkSide       1927\n",
      "...           ...        ...\n",
      "1430      Gilbert       2005\n",
      "1431      NPkVill       1976\n",
      "1432      OldTown       1927\n",
      "1433      Gilbert       2000\n",
      "1434      Mitchel       1977\n",
      "1435        NAmes       1962\n",
      "1436        NAmes       1971\n",
      "1437      NridgHt       2008\n",
      "1438      OldTown       1957\n",
      "1439       NWAmes       1979\n",
      "1440      Crawfor       1922\n",
      "1441      CollgCr       2004\n",
      "1442      Somerst       2008\n",
      "1443      BrkSide       1916\n",
      "1444      CollgCr       2004\n",
      "1445       Sawyer       1966\n",
      "1446      Mitchel       1962\n",
      "1447      CollgCr       1995\n",
      "1448      Edwards       1910\n",
      "1449      MeadowV       1970\n",
      "1450        NAmes       1974\n",
      "1451      Somerst       2008\n",
      "1452      Edwards       2005\n",
      "1453      Mitchel       2006\n",
      "1454      Somerst       2004\n",
      "1455      Gilbert       1999\n",
      "1456       NWAmes       1978\n",
      "1457      Crawfor       1941\n",
      "1458        NAmes       1950\n",
      "1459      Edwards       1965\n",
      "\n",
      "[1460 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1971.267808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.202904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1872.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         YearBuilt\n",
       "count  1460.000000\n",
       "mean   1971.267808\n",
       "std      30.202904\n",
       "min    1872.000000\n",
       "25%    1954.000000\n",
       "50%    1973.000000\n",
       "75%    2000.000000\n",
       "max    2010.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listofcolumns = ['Neighborhood','YearBuilt']  #create a list using column names that are of interest\n",
    "twodatacolumns = data[listofcolumns] # \"splice\" the data using the list of relevant combos\n",
    "print(twodatacolumns) \n",
    "twodatacolumns.describe() #describes the smaller data sheet created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9: Defines the y variable of what value will be predicted (and trained with) as the SalePrice column, which we defined above.\n",
    "\n",
    "#### 10: Defines the x variable dataset as the data[listofxvalues] columns, used to predict the y.\n",
    "\n",
    "#### 11: Imports and creates a Decision Tree Regressor object called iowamodel that can modified and trained to predict values. This prints the values that change how the tree works and optimizes it, although all of these values are set to default as of right now.\n",
    "\n",
    "#### 12: Fits the data defined as x and y to the model, training it to make predictions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "13d533d615e2205d35ac0e804ada0ce0c7c7e64c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = saleprice_data #define your y variable (what will be predicted), we already did this above\n",
    "listofxvalues = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']# the x values being used\n",
    "x = data[listofxvalues] # once again splice the data into the values we are looking for\n",
    "from sklearn.tree import DecisionTreeRegressor #imports the regression system\n",
    "iowamodel = DecisionTreeRegressor() #declare a model object\n",
    "iowamodel.fit(x,y) # train the model using the x and y declared above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13: Prints the x values that will be used to make a prediction, the predictions, and the actual y values. Notice how the values are the same, as the model has been trained using this same testing dataset. This is called bias, as the model isn't actually predicting but \"remembering\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "9f102a7de9f1631585b900ab75bb21bd7b75fab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for the following five houses:\n",
      "   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
      "0     8450       2003       856       854         2             3   \n",
      "1     9600       1976      1262         0         2             3   \n",
      "2    11250       2001       920       866         2             3   \n",
      "3     9550       1915       961       756         1             3   \n",
      "4    14260       2000      1145      1053         2             4   \n",
      "\n",
      "   TotRmsAbvGrd  \n",
      "0             8  \n",
      "1             6  \n",
      "2             6  \n",
      "3             7  \n",
      "4             9  \n",
      "\n",
      "The predictions are:\n",
      "[208500. 181500. 223500. 140000. 250000.]\n",
      "\n",
      "The actual y values are:\n",
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions for the following five houses:')\n",
    "print(x.head()) # top 5 lines of the column\n",
    "print('\\nThe predictions are:')\n",
    "print(iowamodel.predict(x.head())) #using the trained model predict y values from those x value\n",
    "print('\\nThe actual y values are:')\n",
    "print(y.head()) #actual first 5 price values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14: Imports a method that will calculate the average residual for every data point in the model.\n",
    "\n",
    "#### 15: Predicts all values possible from our set of x data.\n",
    "\n",
    "#### 16: Calculates and prints the mean residual for every point of our model, which is exceptionally low, once again due to bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "3cecf3d7f756b8ec3e41f03431769495dde6536d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.35433789954339"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error #import to find mean absolute error (simple residual)\n",
    "predictedvalues = iowamodel.predict(x) #predict A L L of the possibilities\n",
    "mean_absolute_error(y, predictedvalues) #calculate and print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17: Imports and uses a method that randomly splits the x and y data into a training set and a testing set that will help to prevent model bias, but increase the error because of it. \n",
    "\n",
    "#### 18: Creates a new model and trains it with the new testing and training data.\n",
    "\n",
    "#### 19: Prints the mean redisuals for every point of our model which is now much higher, but also less biased. This model is now extrapolating rather than interpolating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33919.509589041096"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #okay so now we are going to split the data set between training and predicting to actually test the effectiveness of the model as well as to prevent biases from interpolation\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, random_state =0) #WHEW okay so we have a tuple of values setup that is equal to this method that uses a RNG to determine which values are in each category\n",
    "iowamodel2 = DecisionTreeRegressor() #creating a new model\n",
    "iowamodel2.fit(train_x, train_y) #training the model using the specified training data\n",
    "predictions =  iowamodel2.predict(val_x) #predicts the values\n",
    "mean_absolute_error(val_y, predictions) #finds the total mean absolute error and prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20: Defines a function MAE (mean absolute error, or mean residuals) that takes a numleaves argument as well as the dataset. \n",
    "\n",
    "#### 21: Creates a model with the numleaves value passed to it, which changes the max_leaf_nodes value. This value changes how many final \"nodes\" there are for the tree to choose from. If there are too many nodes there are only a few examples for each node, meaning this model would be overfit and unreliably predict values, while too few nodes would not allow enough diversity for predictions, leading to an underfit model.\n",
    "\n",
    "#### 22: Returns the MAE of the model created with the given max_leaf_nodes value.\n",
    "\n",
    "#### 23: Uses a for loop to go through a list of values for the max_leaf_nodes values, and find the minimum value among them. At each stage it prints the MAE for the given value.\n",
    "\n",
    "#### 24: Prints the best value found for the max_leaf_nodes value (lowest MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "ae2f0a87c05f878887a652de1e717c3d57bc00ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for 75 is 27455.537405:\n",
      "The MAE for 76 is 27407.400419:\n",
      "The MAE for 77 is 27344.129436:\n",
      "The MAE for 78 is 27258.932587:\n",
      "The MAE for 79 is 27280.232090:\n",
      "The MAE for 80 is 27280.232090:\n",
      "The MAE for 81 is 27241.884768:\n",
      "The MAE for 82 is 27203.783574:\n",
      "The MAE for 83 is 27458.229984:\n",
      "The MAE for 84 is 27458.229984:\n",
      "The MAE for 85 is 27648.298477:\n",
      "The leaf configuration that best fits is: 82 and the MAE calculated is 27203.783573767258\n"
     ]
    }
   ],
   "source": [
    "def mae(max_leaf_nodes, trainx, trainy, valx, valy):#defining a neat function that calculates the average residual for us. used to prevent over or underfitting from the model\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0) #some more fancy schmancy stuff, but basically it creates a model with some specific attributes, notably the number of leaves and the random state agaiuun? dont know what random state is for\n",
    "    model.fit(trainx, trainy) #just training the model, though we could pass a model as an argument if necessary\n",
    "    predicted = model.predict(valx) #once again predicts values\n",
    "    return (mean_absolute_error(valy, predicted))#returns the average residual!\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, random_state =0) #once again splitting the model into test sets \n",
    "min = 999999999 # arbitrarily setting up a very large value to detect a min\n",
    "num = 0 #declaring a variable that will print out the best leaf config\n",
    "for i in [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85]: #for loop through a list of a bunch of possible values. rn i manually\n",
    "    result = mae(i, train_x, train_y, val_x, val_y)#calculates a result through the values\n",
    "    if result < min: #testing to find min value\n",
    "        min = result\n",
    "        num = i\n",
    "    print('The MAE for %s is %f:'%(i, result))\n",
    "print('The leaf configuration that best fits is: %s and the MAE calculated is %s' %(num, min))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25: Imports pandas, time, and various sklearn libraries that will be used for building and analyzing the model.\n",
    "\n",
    "#### 26: Creates a directory string to refer the the train.csv file and creates a DataFrame object from that csv file.\n",
    "\n",
    "#### 27: Creates a y value of the DataFrame that is the SalePrice column, and an x value from the given xlist columns.\n",
    "\n",
    "#### 28: Splits the x and y data into test and train data sets\n",
    "\n",
    "#### 29: Declares some variables(and a dictionary) that will be used to return the optimal model values.\n",
    "\n",
    "#### 30: Goes through a for loop of different n_estimator values to optimize a RandomForest regressor rather than a decision tree. RandomForest essentially averages the results of multiple decision trees, allowing for a more accurate prediction.\n",
    "\n",
    "#### 31: In this for loop, the time spent to calculate the predictions for the model and the minimum value are recorded and printed.  This is because as you increase n_estimators the model becomes more accurate, but it also takes much longer to process and has diminishing returns. n_estimators is essentially the number of decision trees in the random forest.\n",
    "\n",
    "#### 32: Creates a final model using the optimal n_estimators value and trains it with the entire dataset.\n",
    "\n",
    "#### 33: Creates a new string for the test data directory, and then predicts the prices from the given x values. \n",
    "\n",
    "#### 34: The predicted values are put into a DataFrame with the Id and turned into a csv, which is output and turned into Kaggle for scoring\n",
    "\n",
    "#### 35: This model recieved a Root Mean Squared Logarithmic Error of 0.19069, which is the top 72%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "1d345beee1565492a724981e6fa364586f435b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 5 \t MAE: 25330.81 \t Time spent in seconds to calculate: 0.03\n",
      "n_estimators: 25 \t MAE: 23591.65 \t Time spent in seconds to calculate: 0.08\n",
      "n_estimators: 50 \t MAE: 23242.59 \t Time spent in seconds to calculate: 0.17\n",
      "n_estimators: 75 \t MAE: 22964.55 \t Time spent in seconds to calculate: 0.25\n",
      "n_estimators: 100 \t MAE: 23093.06 \t Time spent in seconds to calculate: 0.34\n",
      "n_estimators: 200 \t MAE: 23007.88 \t Time spent in seconds to calculate: 0.65\n",
      "n_estimators: 500 \t MAE: 22907.49 \t Time spent in seconds to calculate: 1.66\n",
      "n_estimators: 1000 \t MAE: 22909.57 \t Time spent in seconds to calculate: 3.26\n",
      "n_estimators: 2000 \t MAE: 22897.42 \t Time spent in seconds to calculate: 6.45\n",
      "n_estimators: 5000 \t MAE: 22946.20 \t Time spent in seconds to calculate: 16.47\n",
      "The best fit is an n_estimators value of 2000 with an MAE of 22897.42, and a time spent calculating of 6.45\n",
      "        Id      SalePrice\n",
      "0     1461  121550.027500\n",
      "1     1462  155215.732000\n",
      "2     1463  183307.725500\n",
      "3     1464  178695.345000\n",
      "4     1465  187554.218500\n",
      "5     1466  182530.208500\n",
      "6     1467  172034.398500\n",
      "7     1468  174431.388500\n",
      "8     1469  190225.981500\n",
      "9     1470  115569.656000\n",
      "10    1471  189907.909000\n",
      "11    1472   93429.728571\n",
      "12    1473   90885.108333\n",
      "13    1474  144861.007000\n",
      "14    1475  125732.668405\n",
      "15    1476  323603.639000\n",
      "16    1477  245513.105500\n",
      "17    1478  276044.833000\n",
      "18    1479  333591.023000\n",
      "19    1480  453528.977500\n",
      "20    1481  314215.724000\n",
      "21    1482  203650.725500\n",
      "22    1483  202758.019500\n",
      "23    1484  164398.395000\n",
      "24    1485  173339.290000\n",
      "25    1486  207448.420000\n",
      "26    1487  282236.427000\n",
      "27    1488  250997.725500\n",
      "28    1489  199188.553500\n",
      "29    1490  238174.335500\n",
      "...    ...            ...\n",
      "1429  2890   82913.205500\n",
      "1430  2891  141249.894000\n",
      "1431  2892  103344.150000\n",
      "1432  2893  120833.885357\n",
      "1433  2894   78923.921500\n",
      "1434  2895  295954.008000\n",
      "1435  2896  269585.545500\n",
      "1436  2897  197413.860000\n",
      "1437  2898  136480.738350\n",
      "1438  2899  220304.128500\n",
      "1439  2900  160795.696167\n",
      "1440  2901  234200.400000\n",
      "1441  2902  191044.514500\n",
      "1442  2903  367104.007000\n",
      "1443  2904  307746.492500\n",
      "1444  2905  186885.162000\n",
      "1445  2906  204407.345000\n",
      "1446  2907  120982.247000\n",
      "1447  2908  124082.240000\n",
      "1448  2909  139163.404333\n",
      "1449  2910   81064.750000\n",
      "1450  2911   93619.251786\n",
      "1451  2912  149742.655000\n",
      "1452  2913   89285.183333\n",
      "1453  2914   89145.483333\n",
      "1454  2915   83897.491667\n",
      "1455  2916   87476.200000\n",
      "1456  2917  155757.312500\n",
      "1457  2918  124403.350000\n",
      "1458  2919  227485.470000\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#This is me using the techniques I've learned throughout this course in one block to show how to create these models\n",
    "import pandas as pd\n",
    "import time #going to be used for time spent calcs\n",
    "from sklearn.ensemble import RandomForestRegressor #importing a different regression method\n",
    "from sklearn.metrics import mean_absolute_error # import residual function\n",
    "from sklearn.model_selection import train_test_split #splits data\n",
    "\n",
    "directory = 'train.csv' #once again declaring a directory variable as string\n",
    "data = pd.read_csv(directory) #create a data frame using pandas from the csv file\n",
    "\n",
    "y = data.SalePrice # uses dot notation to identify what we are predicting\n",
    "xlist = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd'] #defines an x list of columns/features to be used\n",
    "x = data[xlist] #creating our x data using slicing with our list\n",
    "\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, random_state = 0) #splits the data into test and train sets\n",
    "min = 99999999999999\n",
    "g = 0\n",
    "t2 = {}\n",
    "for i in (5, 25, 50, 75, 100, 200, 500, 1000, 2000, 5000): #tests different values for n_estimators, explained in the comments below\n",
    "    start = time.time()\n",
    "    model = RandomForestRegressor(n_estimators = i, random_state =0) #instantiate model using a different value, still unsure as to what random_state does\n",
    "    model.fit(train_x, train_y) # fit the model with training data\n",
    "    predicted = model.predict(val_x) #define predicted value\n",
    "    mae = mean_absolute_error(val_y, predicted)\n",
    "    if(mae < min):\n",
    "        min = mae\n",
    "        g = i\n",
    "    end = time.time()\n",
    "    t = end-start\n",
    "    t2[i] = t\n",
    "    print('n_estimators: %s \\t MAE: %0.2f \\t Time spent in seconds to calculate: %0.2f'%(i, mae, t)) # print the MAE (avergae residual) \n",
    "print('The best fit is an n_estimators value of %s with an MAE of %0.2f, and a time spent calculating of %0.2f'%(g, min, t2[g]))\n",
    "\n",
    " #http://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    " #The main parameters to adjust when using these methods is n_estimators and max_features. \n",
    " #The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. \n",
    " #In addition, note that results will stop getting significantly better beyond a critical number of trees. \n",
    " #The latter is the size of the random subsets of features to consider when splitting a node. \n",
    " #The lower the greater the reduction of variance, but also the greater the increase in bias. \n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 2000, random_state =0) #building a final model\n",
    "model.fit(x, y) #fitting the model with data, no splitting\n",
    "testdata = pd.read_csv('test.csv') #declaring the testdata as a separate csv\n",
    "testx = testdata[xlist] \n",
    "predictedprices = model.predict(testx)\n",
    "submission = pd.DataFrame({'Id':testdata.Id, 'SalePrice':predictedprices})\n",
    "submission.to_csv('submisson.csv', index=False)\n",
    "print(submission)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "862ce547a0061d37587c8efaf6c550ff7970ff71",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
